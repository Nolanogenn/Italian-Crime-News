install.packages('tokenizers')
library(tokenizers)
data_full <- read.csv('/home/gennaronolano/Progetti/unimore/Italian-Crime-News/italian_crime_news.csv')
names(data_full)
text <- data_full$text
corpus <- paste(sentences, collapse=' ')
corpus <- paste(text, collapse=' ')
tokens_all <- tokenize_words(corpus)[[1]]
tokens_unique <- unique(tokens_all)
set_labels
set_labels <- unique(data_full$newspaper_tag)
text <- data_full$text
corpus <- paste(text, collapse=' ')
tokens_all <- tokenize_words(corpus)[[1]]
tokens_unique <- unique(tokens_all)
set_labels
for (num in 1:length(set_labels)){
label <- set_labels[num]
corpus_y <- tokenize_words(paste(data_full$text[which(data_full$newspaper_tag=label)], collapse=' '))[[1]]
corpus_n <- tokenize_words(paste(data_full$text[which(data_full$newspaper_tag!=label)], collapse=' '))[[1]]
column <- c()
for (token in tokens_unique){
lodd <- log_odds(token, corpus_y, corpus_n, tokens_all)
append(column, lodd)
}
all_lodds[,num] <- column
}
all_lodds <- data.frame(matrix(ncol=length(set_labels),nrow=0))
for (num in 1:length(set_labels)){
label <- set_labels[num]
corpus_y <- tokenize_words(paste(data_full$text[which(data_full$newspaper_tag=label)], collapse=' '))[[1]]
corpus_n <- tokenize_words(paste(data_full$text[which(data_full$newspaper_tag!=label)], collapse=' '))[[1]]
column <- c()
for (token in tokens_unique){
lodd <- log_odds(token, corpus_y, corpus_n, tokens_all)
append(column, lodd)
}
all_lodds[,num] <- column
}
for (num in 1:length(set_labels)){
label <- set_labels[num]
corpus_y <- tokenize_words(paste(data_full$text[which(data_full$newspaper_tag=label)], collapse=' '))[[1]]
corpus_n <- tokenize_words(paste(data_full$text[which(data_full$newspaper_tag!=label)], collapse=' '))[[1]]
column <- c()
for (token in tokens_unique){
lodd <- log_odds(token, corpus_y, corpus_n, tokens_all)
append(column, lodd)
}
all_lodds[,num] <- column
}
print(num)
log_odds <- function(token, corpus_i, corpus_j, background_corpus){
w_i <- length(which(corpus_i == token))
w_j <- length(which(corpus_j == token))
alpha_w <- length(which(background_corpus == token))
n_i <- length(corpus_i)
n_j <- length(corpus_j)
alpha_zero <- length(background_corpus)
num <- log((w_i + alpha_w) / (n_i + alpha_zero - w_i - alpha_w)) - log((w_j + alpha_w) / (n_j + alpha_zero - w_j - alpha_w))
dirichlet_prior <- (1/(n_i + alpha_w)) + (1/(n_j + alpha_w))
denum <- sqrt(dirichlet_prior)
lodd <- num / denum
return(lodd)
}
all_lodds <- data.frame(matrix(ncol=length(set_labels),nrow=0))
for (num in 1:length(set_labels)){
label <- set_labels[num]
corpus_y <- tokenize_words(paste(data_full$text[which(data_full$newspaper_tag=label)], collapse=' '))[[1]]
corpus_n <- tokenize_words(paste(data_full$text[which(data_full$newspaper_tag!=label)], collapse=' '))[[1]]
column <- c()
for (token in tokens_unique){
lodd <- log_odds(token, corpus_y, corpus_n, tokens_all)
append(column, lodd)
}
all_lodds[,num] <- column
}
corpus_y <- tokenize_words(paste(data_full$text[which(data_full$newspaper_tag==label)], collapse=' '))[[1]]
for (num in 1:length(set_labels)){
label <- set_labels[num]
corpus_y <- tokenize_words(paste(data_full$text[which(data_full$newspaper_tag==label)], collapse=' '))[[1]]
corpus_n <- tokenize_words(paste(data_full$text[which(data_full$newspaper_tag!=label)], collapse=' '))[[1]]
column <- c()
for (token in tokens_unique){
lodd <- log_odds(token, corpus_y, corpus_n, tokens_all)
append(column, lodd)
}
all_lodds[,num] <- column
}
